{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtrouRqjr1E"
   },
   "source": [
    "# Learning promtp engineering from GPT-4\n",
    "### notebook version\n",
    "\n",
    "raul.arrabales | Dec. '23\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhBnYAaj_MB",
    "tags": []
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGpQEj0Qf3oU",
    "outputId": "4a1ed18f-8cda-4d07-d450-cc819350dabd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Already satisfied in my container\n",
    "# ! pip install python-dotenv\n",
    "# ! pip install tiktoken\n",
    "# ! pip install openai\n",
    "# ! pip install panel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJIGNfhTkHNw",
    "tags": []
   },
   "source": [
    "## Connect to my data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kl7z3kNdr1F",
    "outputId": "0a3d2d12-abfb-4e30-e822-764ce562a8f6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using my local container data. \n",
    "# Use the following to use GDrive: \n",
    "\n",
    "# Mount my drive folder\n",
    "# Only for Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGqlNg9NkM5B",
    "tags": []
   },
   "source": [
    "## Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bNfTOZKwcJ9C",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYG-kz5QkRut",
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "n6IpVtdLcKAO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# _ = load_dotenv('/content/drive/My Drive/Colab Notebooks/.env') # read drive colab folder .env file\n",
    "_ = load_dotenv('myenv.txt')\n",
    "\n",
    "openai.api_key  = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDD15ta_kdTt",
    "tags": []
   },
   "source": [
    "## OpenAI API Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a7dff174",
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' *** get_completion(prompt, model, temp) ***\n",
    "        -----------------------------------\n",
    "Helper function to get a GPT 3.5 Turbo completion\n",
    "Using the chat completion API\n",
    "(https://platform.openai.com/docs/guides/gpt/chat-completions-api)\n",
    "\n",
    "INPUT:\n",
    "- prompt: user's prompt\n",
    "- model: OpenAI model (GPT 3.5 Turbo by default)\n",
    "- temp: model's temperature (0 by default)\n",
    "\n",
    "OUTPUT:\n",
    "- The instruct LLM response\n",
    "'''\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temp=0):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AY1bnHDhn7Sn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' *** get_completion_from_messages(messages, model, temp) ***\n",
    "        ---------------------------------------------------\n",
    "Helper function to get a GPT 3.5 Turbo completion.\n",
    "In this case, from the list of past messages.\n",
    "Using the chat completion API\n",
    "(https://platform.openai.com/docs/guides/gpt/chat-completions-api)\n",
    "\n",
    "INPUT:\n",
    "- messages: list of messages\n",
    "- model: OpenAI model (GPT 3.5 Turbo by default)\n",
    "- temp: model's temperature (0 by default)\n",
    "\n",
    "OUTPUT:\n",
    "- The instruct LLM response\n",
    "'''\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temp=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temp,\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' *** chat_with_context(prompt) ***\n",
    "        -----------------------------\n",
    "Gets completion using context global variable as the list of messages. \n",
    "Updates the context with current messages. \n",
    "\n",
    "INPUT:\n",
    "- prompt: user's prompt\n",
    "\n",
    "OUTPUT:\n",
    "- Last LLM's response\n",
    "'''\n",
    "def chat_with_context(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = get_completion_from_messages(context, model=model)\n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb \n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "        }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n",
    "        )\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKtvD-R1m7IB",
    "tags": []
   },
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## First conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Starting with an empty context\n",
    "context = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Getting responses from GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_role_prompt = f\"\"\"\n",
    "You are an expert in LLMs and prompt engineering.\n",
    "\"\"\"\n",
    "# List of messages, starting with system's role\n",
    "context.append({'role':'system', 'content':system_role_prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_0 = f\"\"\"\n",
    "What is prompt engineering?. \n",
    "\"\"\"\n",
    "context.append({'role':'user', 'content':f\"{prompt_0}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_0 = get_completion_from_messages(context, model='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the process of carefully crafting inputs (prompts) to elicit desired outputs from language models, particularly large language models (LLMs) like OpenAI's GPT-3 or GPT-4, Google's BERT, or other AI systems. It involves understanding how the model interprets and responds to different types of language and structuring prompts to guide the model towards generating useful, accurate, and contextually appropriate responses.\n",
      "\n",
      "Prompt engineering is both an art and a science, as it requires a blend of creativity, linguistic skill, and technical understanding of the model's capabilities and limitations. Effective prompt engineering can significantly enhance the performance of a language model, making it more applicable to a wide range of tasks such as content creation, code generation, information retrieval, conversation, and more.\n",
      "\n",
      "Key aspects of prompt engineering include:\n",
      "\n",
      "1. **Clarity**: Ensuring the prompt is clear and unambiguous to avoid confusion.\n",
      "2. **Context**: Providing sufficient background information so the model can generate relevant responses.\n",
      "3. **Conciseness**: Keeping the prompt succinct to maintain focus and efficiency.\n",
      "4. **Instruction**: Including explicit instructions when necessary to guide the model's output.\n",
      "5. **Creativity**: Experimenting with different formulations to discover the most effective ways to communicate with the model.\n",
      "6. **Iteration**: Refining prompts based on the model's responses to improve outcomes.\n",
      "\n",
      "Prompt engineering is particularly important when using LLMs for specific tasks that require precision, such as technical writing, coding, or legal analysis. By fine-tuning the prompts, engineers and users can better leverage the AI's capabilities to suit their needs.\n"
     ]
    }
   ],
   "source": [
    "print(response_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_1 = f\"\"\"\n",
    "What is meta prompting?\n",
    "\"\"\"\n",
    "context.append({'role':'user', 'content':f\"{prompt_1}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_1 = get_completion_from_messages(context, model='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the process of carefully crafting inputs (prompts) for language models, like GPT-3 or other large language models (LLMs), to elicit the best possible outputs. It involves understanding how the model interprets and responds to different types of prompts, and using this knowledge to design prompts that guide the model towards producing the desired text.\n",
      "\n",
      "Prompt engineering can include:\n",
      "\n",
      "1. **Precision in Language**: Using specific and clear language to reduce ambiguity in the prompt.\n",
      "2. **Contextual Information**: Providing sufficient background information to help the model understand the context of the request.\n",
      "3. **Instructional Prompts**: Giving direct instructions to the model about what kind of information or response is expected.\n",
      "4. **Examples**: Including examples in the prompt to illustrate the type of response desired.\n",
      "5. **Iterative Refinement**: Adjusting prompts based on the outputs received to improve the quality of future responses.\n",
      "\n",
      "Meta prompting, on the other hand, is a more advanced form of prompt engineering that involves creating prompts that are self-referential or that instruct the language model to reflect on its own processes or capabilities. This can include asking the model to explain how it might approach a problem, to detail its limitations, or to consider how it would optimize its own performance for a given task.\n",
      "\n",
      "Meta prompting can be particularly useful for:\n",
      "\n",
      "1. **Debugging**: Understanding why a model is producing certain outputs.\n",
      "2. **Model Exploration**: Discovering the boundaries of what the model knows and can do.\n",
      "3. **Teaching the Model**: Guiding the model to learn new patterns or approaches through self-analysis.\n",
      "4. **Optimization**: Helping the model to refine its responses for more complex tasks.\n",
      "\n",
      "Both prompt engineering and meta prompting are important skills for anyone looking to leverage the full potential of LLMs, as they can significantly impact the effectiveness and efficiency of the model's outputs.\n"
     ]
    }
   ],
   "source": [
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_2 = f\"\"\"\n",
    "Can you give me an example of meta prompting applied to an insurance use case where I need the LLM \\\n",
    "to determine to what extent a specific claim is covered by the particular insurance policy conditions \\\n",
    "of the user? (I can include in the prompt both the claim description and policy conditions)\n",
    "\"\"\"\n",
    "context.append({'role':'user', 'content':f\"{prompt_2}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_2 = get_completion_from_messages(context, model='gpt-4-1106-preview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta prompting involves creating prompts that are self-referential or that instruct the language model to reflect on its own processes, capabilities, or the nature of the task it's being asked to perform. It's a way to guide the model to better understand the context or the meta-information about the task at hand.\n",
      "\n",
      "In the insurance use case you've described, meta prompting could be used to help the language model understand that it needs to interpret the insurance policy conditions and apply them to the specific claim description to determine coverage. Here's an example of how you might structure such a meta prompt:\n",
      "\n",
      "---\n",
      "\n",
      "**Prompt:**\n",
      "\n",
      "\"Language model, you are to act as an assistant to an insurance claims adjuster. Your task is to analyze the text of an insurance policy and a description of a claim to determine the extent of coverage. You should consider yourself an expert system that can parse legal and insurance-related language, identify relevant clauses, and apply them to specific scenarios. Please note that your analysis should not be considered final or legally binding, but rather a preliminary assessment to assist in the claims process.\n",
      "\n",
      "Below are the details of the insurance policy conditions and the claim description. Please review them carefully and provide a detailed explanation of whether the claim is covered by the policy, citing the specific policy conditions that support your assessment.\n",
      "\n",
      "**Insurance Policy Conditions:**\n",
      "[Insert the detailed conditions of the insurance policy here.]\n",
      "\n",
      "**Claim Description:**\n",
      "[Insert the detailed description of the claim here.]\n",
      "\n",
      "Based on the information provided, please explain to what extent the specific claim is covered by the insurance policy conditions.\"\n",
      "\n",
      "---\n",
      "\n",
      "This meta prompt sets the stage for the language model by defining its role and the nature of the task. It also reminds the model to provide explanations and cite specific policy conditions, which is crucial for transparency and for the adjuster to understand the reasoning behind the coverage determination.\n"
     ]
    }
   ],
   "source": [
    "print(response_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
