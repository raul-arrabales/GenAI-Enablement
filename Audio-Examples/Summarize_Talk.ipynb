{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29cb179e-4ab8-4a97-bb66-bd793a45175f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Summarize Talk Notebook\n",
    "\n",
    "**Input:** Audio file containing speech.<br>\n",
    "**Output:** A brief summary of what is said in the talk.<br>\n",
    "\n",
    "---\n",
    "Ra√∫l Arrabales Moreno - raul.arrabales@gmail.com (Jun. 2023)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb7ad5-5eed-471d-892b-5eb4dc9e8a06",
   "metadata": {},
   "source": [
    "This notebook utilizes:\n",
    "- OpenAI Whisper for transcription (ASR - Speech to Text). \n",
    "    - https://github.com/openai/whisper\n",
    "- OpenAI Davinci for text summarization. \n",
    "    - https://platform.openai.com/docs/models/gpt-3-5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e7872b-c816-4349-a00e-9a698ed62195",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb6d650-9a7b-4375-8f08-88127eb96ea1",
   "metadata": {},
   "source": [
    "### Input file specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46ff2b94-8b3c-483e-8486-8735500a8f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A crappy audio file with bad quality and low volume\n",
    "# from myself performing a demo of quering datasets using natural language\n",
    "# See https://www.youtube.com/watch?v=prbl_cM3iJk \n",
    "audio_file_path = \"data/Quering_Datasets_NLU.mp3\"\n",
    "\n",
    "# Where to save the transcription\n",
    "text_file_path = \"data/QD_NLU_transcription.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bdd7d4-ded6-46d7-95d1-242ff76543c8",
   "metadata": {},
   "source": [
    "### Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c74e42f1-b152-490c-a2d1-8140e5512291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OS interfacing\n",
    "import os \n",
    "\n",
    "# for counting tokens\n",
    "import tiktoken\n",
    "\n",
    "# OpenAI Client\n",
    "import openai \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95301315-fa0b-4721-813b-8218320bc5dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### OpenAI API Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fd4724b-8f1f-4701-a607-83939b1a86da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# My OpenAI API key is in a file called 'ram_openai_apikey.txt', \n",
    "# in the same directory as this notebook\n",
    "key_filename = 'ram_openai_apikey.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e863cbb-29de-4fe7-9fde-dc569bcacbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_apikey_from_file(filename):\n",
    "    \"\"\" Given a filename,\n",
    "        return the contents of that file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            # It's assumed our file contains a single line,\n",
    "            # with our API key\n",
    "            return f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        print(\"'%s' file not found\" % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a0f1d98-4ada-4cf9-9566-921b0bc36dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "openai_api_key = get_apikey_from_file(key_filename)\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e0468-29e8-481e-b339-826f06506b45",
   "metadata": {},
   "source": [
    "## Audio Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "063b9a13-919b-4f02-a884-321aa5ebcbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the audio file\n",
    "audio_file = open(audio_file_path, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b4fc2af-60b9-41fd-b582-2ef97004a988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Whisper to transcribe the speech into text\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "203ddb2a-754b-4644-8a3f-e67aabd58022",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the transcription to disk (for further use)\n",
    "with open(text_file_path, 'w') as f:\n",
    "    f.write(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1dd9167-7e8a-4ce5-ae40-355202a10abf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello everyone, Raul speaking. This is a brief description of this project querying datasets using natural language. So the basic idea behind this small application is just to have a chat, like a chatbot, in order to have an interface for a dataset. So imagine that you are, let's say, a psychologist, that you don't have any expertise working with programming languages, you don't know Python, you don't know SQL, but you need to get some insights or some analytics from a dataset, from a data file. So let me show you the application, how it works. So if I jump into the application directly, the first thing is I'm able to browse, select a dataset, a CSV file. So if I open this one, basically this comes from an experiment I ran. It has something to do with a psychological experiment, I'm not going to explain that here, but the important thing is we have several rows, we have columns, we can identify things like gender, age, some kind of code. If I scroll here horizontally, I see some text in Spanish, some other things. So what's the value of this application? As I said, we can go and start asking questions to the chatbot about this dataset. For instance, let's ask about rows and columns. Submit. So the thing is I don't need to know any SQL or any programming language in order to query the dataset. So what is happening under the hood is that the agent I'm using is translating my question in English here into Python and then it's executing the Python code through Pandas using the dataset I provided and then internally it gets a reply in Python and again translates that into a natural language. So I get the final answer that a friend has whatever number of rows and columns. So let's try maybe more interesting things. So let's say, okay, what is the average for task 20? Task 20 is one of the columns here. Let's say I want to calculate what is the average. So 67 point something. So again, very easy to do, I don't need any knowledge of any programming language or anything. And furthermore, we could try something more complex. What are the most common values in Yandere? So I have a column Yandere, but looking at the file, it is two is the most common value. Okay, but I want, yeah, here this is an example of where the body is not actually understanding what I want because I want the list of values. So, okay, what are, what is the list of different values in Yandere? Okay, so basically if you know Python, I'm asking for this list, two, one, and three. So now, I know this from the experiment because I was the one building this file. So let's say if one represents male, two represents female, and three others, what is the percentage of each Yandere in the dataset? Okay, so I'm asking here, okay, I want to know how, you know, every Yandere is represented in this dataset. So here, what you can see is very easy that you give context information to the bot, in this case the coding for the Yandere column, and it is able to automatically calculate the percentage of the representation of each Yandere in the file. So what I can see here is females are overrepresented, males are underrepresented here. So yeah, this is one example of how one could use this application. Hope you can find this useful. Maybe I can prepare another video explaining the details of how this has been built. Okay, thank you very much for your attention.\n"
     ]
    }
   ],
   "source": [
    "# See the transcription\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f5e9809-44e2-4def-8d55-2342913d9530",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "750"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of tokens\n",
    "encoding = tiktoken.encoding_for_model(\"text-davinci-003\") # Encoder for davinci model\n",
    "len(encoding.encode(transcript.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06829a2e-fe1b-451b-a8ce-8a9eceeb76a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Summarizing the talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63c9d8d3-1e23-42f7-b4ab-44bd7f332099",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask davinci to summarize with \"Tl;dr\"\n",
    "# See https://platform.openai.com/examples/default-tldr-summary \n",
    "response = openai.Completion.create(\n",
    "  model=\"text-davinci-003\",\n",
    "  prompt=f'{transcript.text}\\n\\nTl;dr',\n",
    "  temperature=1,\n",
    "  max_tokens=240,\n",
    "  top_p=1.0,\n",
    "  frequency_penalty=0.0,\n",
    "  presence_penalty=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7f99b587-12fa-44cf-8371-43bc41277be0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This project is an application, which allows users to query datasets using natural language. The app allows users to select a CSV file and then use the chatbot to ask questions about the data set in the form of regular sentences. Under the hood, the bot translates these sentences into Python code and runs the queries on the dataset using Pandas. This lets non-programmers interact with data sets and get results without any programming knowledge. Examples are provided to explore this process, such as working out the list of different values in one column and their percentages in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc5991-d80e-4015-9e5e-02ee2e80fd4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5887eeb-8955-4fba-bcd5-e294f2d8b31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58928e22-2170-47e3-ba22-47d9dd99bdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
